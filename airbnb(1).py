# -*- coding: utf-8 -*-
"""airbnb.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZHknVPY1yMcgwSmLdusi14_SMiYz2qa3
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from google.colab import files
uploaded=files.upload()
for i in uploaded.keys():
  print('user uploaded"{name}" with length {length} bytes'.format(name=i,length=len(uploaded[i])))

file="data_clean_airbnb.xlsx"
data=pd.read_excel(file)

data.head()

data.columns

"""**** my dataset into numerical  and categorical subfields and assigning them to different variables ****"""

d1=data.select_dtypes(include=np.number).columns
print(d1)
print(data.loc[:,'country code':'service fee'])

d2=data.select_dtypes(include='object').columns
print(d2)

"""*** Checking length of my newly formed lists and data type to affirm the authenticity for data analysis***"""

len(data.columns)

len(d1)

len(d2)

d1.dtype

data.columns.dtype

d2.dtype

"""# checking the null values of my dataset"""

data.isna().sum()

"""***23 data fields(columns) have null values in them***

** To get the unique values present in each column to get better idea of dataset **
"""

for i in data.columns:
  print(data[i].value_counts())

"""# Replacing null values of categorical fields with the mode values of respective fields"""

data['NAME'].fillna(data['NAME'].mode()[0],inplace=True)
data['host_identity_verified'].fillna(data['host_identity_verified'].mode()[0],inplace=True)
data['host name'].fillna(data['host name'].mode()[0],inplace=True)
data['neighbourhood group'].fillna(data['neighbourhood group'].mode()[0],inplace=True)
data['neighbourhood'].fillna(data['neighbourhood'].mode()[0],inplace=True)
data['country'].fillna(data['country'].mode()[0],inplace=True)
data['country code'].fillna(data['country code'].mode()[0],inplace=True)
data['cancellation_policy'].fillna(data['cancellation_policy'].mode()[0],inplace=True)
data['instant_bookable'].fillna(data['instant_bookable'].mode()[0],inplace=True)
data['Construction year'].fillna(data['Construction year'].mode()[0],inplace=True)
data['house_rules'].fillna(data['house_rules'].mode()[0],inplace=True)
data['license'].fillna(data['license'].mode()[0],inplace=True)
data['last review'].fillna(data['last review'].mode()[0],inplace=True)

""" ***I am going to check null values after dealing with them everytime to check effectiveness of method***"""

data.isna().sum()

data.head()

"""# Replacing null values of numerical fields with the mean values of respective fields"""

data['lat'].fillna(data['lat'].mean(),inplace=True)
data['long'].fillna(data['long'].mean(),inplace=True)
data['minimum nights'].fillna(data['minimum nights'].mean(),inplace=True)
data['number of reviews'].fillna(data['number of reviews'].mean(),inplace=True)
data['reviews per month'].fillna(data['reviews per month'].mean(),inplace=True)
data['review rate number'].fillna(data['review rate number'].mean(),inplace=True)
data['calculated host listings count'].fillna(data['calculated host listings count'].mean(),inplace=True)
data['availability 365'].fillna(data['availability 365'].mean(),inplace=True)

data.head()

"""** First I have to remove $ sign and then after converting 'service fee' and 'price' columns into numeric columns from object type I will replace the NaN values with means of their respective columns**"""

data['price']=data['price'].str.replace('$','')
data['service fee']=data['service fee'].str.replace('$','')

print(data['price'],data['service fee'])

"""Converting 'service fee' and 'price' columns into numeric columns from object type"""

data['service fee']=pd.to_numeric(data['service fee'],errors='coerce')
data['service fee'].fillna(data['service fee'].mean(),inplace=True)

data['price']=pd.to_numeric(data['price'],errors='coerce')
data['price'].fillna(data['price'].mean(),inplace=True)

data.head()

"""As I cannot see every column of my dataset clearly I am printing the missing fields in next line"""

data.loc[:,'country':'minimum nights']

"""# Dropping duplicate values from my dataset"""

for i in data.columns:
  df=data.duplicated(i)
  print(df.sum(),'\n',data[i])

data.duplicated().sum()

df1=data.drop_duplicates()

df1.head()

"""# Getting unique values from every row of my dataset"""

for j in d2:
  print(df1[j].value_counts())

"""# Replacing bad data"""

df1['neighbourhood group']=df1['neighbourhood group'].replace({'manhatan':'Manhattan','brookln':'Brooklyn'})

df1['last review']=df1['last review'].replace({'2019-01-07 00:00:00' : '1/07/2019','2019-07-07 00:00:00' : '7/07/2019'})

"""# Finding correlation between data fields and plotting heatmap"""

df1.select_dtypes(include=np.number).corr()

sns.heatmap(df1.select_dtypes(include=np.number).corr(),center=0.4,cmap='crest')

"""# Dealing with outliers"""

for i in d1:
  q1=df1[i].quantile(0.25)
  q3=df1[i].quantile(0.75)
  iqr=q3-q1
  outlier_l=q1-1.5*iqr
  outlier_u=q3+1.5*iqr
  outliers=df1[((df1[i]<outlier_l)|(df1[i]> outlier_u))]

  print(outliers[i])

"""latitude has 144 0utliers
longitude has 1893 outliers
number of reviews has 1385 outliers
reviews per month has 4861 outliers
calculated host listing counts has 18021 outliers
"""

dg=pd.DataFrame()

for i in d1:
  q1=df1[i].quantile(0.25)
  q3=df1[i].quantile(0.75)
  iqr=q3-q1
  outlier_l=q1-1.5*iqr
  outlier_u=q3+1.5*iqr
  outliers=df1[((df1[i]<outlier_l)|(df1[i]> outlier_u))]
  print(outliers[i])


  if not outliers.empty:
    df1.loc[outliers.index,i]=df1[i].mean()
dg=df1.copy()
print(dg)

print(dg)

"""# Boxplot to display distribution of data"""

for i in d1:
  sns.boxplot(dg[i])
  plt.show()
  print(i)

dg.isna().sum()

"""Checking distribution of data for price and service fee to know the usual $ that is charged"""

sns.histplot(dg['price'])
plt.show()

plt.show()
sns.kdeplot(dg['service fee'])
plt.show()

sns.set(font_scale=2)

"""Checking relation of service fees and contruction years alongwith price vs construction year

although service distribution is uniform , the probability distribution of price is highest between 400 and 600 so this might be most general price range of airbnb in US
"""

sns.barplot(dg,y=dg['service fee'],x=dg['Construction year'])
sns.set(font_scale=1.5)
sns.set(rc={'figure.figsize':(30,20)})

plt.xticks(rotation=90)
sns.set(font_scale=1.5)
plt.show()
sns.barplot(dg,y=dg['price'],x=dg['Construction year'])
plt.xticks(rotation=90)
sns.set(font_scale=1.5)
plt.show()
sns.lineplot(dg,y=dg['service fee'],x=dg['Construction year'])
plt.xticks(rotation=90)
sns.set(font_scale=1.5)
plt.show()
sns.lineplot(dg,y=dg['price'],x=dg['Construction year'])
plt.xticks(rotation=90)
sns.set(font_scale=1.5)
plt.show()

sns.pairplot(dg)
plt.xticks(rotation=90)
plt.yticks(rotation=90)

"""from all the plots and heatmap and correlation we can infer that there are linear relationships between (price and service fee),(reviews per month and number of reviews) and(latitude and longitude)"""

print(d1,d2)

"""# Dataset is cleaned now so I will export my excel file"""

dg.to_excel('cleaned_airbnb_data2.xlsx')

dg.describe()

"""# Analyzing  and plotting dataset on basis of neighbourhood to see if neighbourhood affects the parameters like price ,service fees with regards to their aggregate functions to check relationships among them"""

daa=dg.groupby(['neighbourhood group'])['price'].sum()
for i,j in daa.items():
  print(i)
  print(j)

sns.barplot(x=dg['neighbourhood'],y=dg['price'])
plt.xticks(rotation=90)
sns.set(rc={'figure.figsize':(100,30)})
sns.set(font_scale=1.2)
plt.show()

dab=dg.groupby(['neighbourhood group'])['price'].mean()
for i,j in dab.items():
  print(i)
  print(j)
dac=dg.groupby(['neighbourhood group'])['service fee'].mean()
for i,j in dac.items():
  print(i)
  print(j)

"""# plotting my grouped data"""

dab.plot()
sns.set(rc={'figure.figsize':(60,30)})
sns.set(font_scale=5)

"""displaying mean price across different neigbourhoods

"""

dac.plot()
sns.set(rc={'figure.figsize':(60,30)})
sns.set(font_scale=5)

"""displaying mean service fee across different neigbourhoods

# Percentages of service fees with respect of prices of residences regionwise and keeping aggregate functions in consideration
"""

dad = dg.groupby(['neighbourhood group'])[['price','service fee']].mean()
dad.plot(kind='bar',stacked=True)
sns.set(rc={'figure.figsize':(60,30)})
sns.set(font_scale=1)

dada=dg.groupby(['neighbourhood group'])['service fee'].apply(lambda x:np.mean((x/dg['price'])*100)).reset_index()
for i,j in dada.items():
  print(i)
  print(j)

dada1=dg.groupby(['neighbourhood group'])['service fee'].apply(lambda x:np.max((x/dg['price'])*100)).reset_index()
for i,j in dada1.items():
  print(i)
  print(j)

dada7=dg.groupby(['neighbourhood group'])['service fee'].apply(lambda x:np.std((x/dg['price'])*100)).reset_index()
for i,j in dada7.items():
  print(i)
  print(j)

dada72=dg.groupby(['neighbourhood group'])['service fee'].apply(lambda x:np.min((x/dg['price'])*100)).reset_index()
for i,j in dada72.items():
  print(i)
  print(j)

"""*checking distribution of percentages*"""

((dg['service fee']/dg['price'])*100).plot(kind='hist',bins=50)
plt.xticks(np.linspace(0,100,5))
plt.show()

"""from this graph and previous groupby function we can infer that service fee charged in all of us usually ranges from 16-32 with few places having exceptions"""

dg['host name'].value_counts()

dg['NAME'].value_counts()

"""# Checking effect cancellation policy on review rate number and reviews per month"""

dg['room type'].value_counts()

dg['cancellation_policy'].value_counts()

dg['review rate number'].value_counts(

)

daba=dg.groupby(['cancellation_policy'])[['review rate number','reviews per month']].mean()
for i,j in daba.items():
  print(i)
  print(j)
daba.plot(kind='bar')
plt.yticks(np.linspace(1,6,6))
plt.rcParams['font.size'] = 5

"""# Checking effect room type on review rate number and reviews per month"""

daba1=dg.groupby(['room type'])[['review rate number','reviews per month']].mean()
for i,j in daba1.items():
  print(i)
  print(j)
daba.plot(kind='bar')
plt.rcParams['font.size'] = 5
plt.yticks(np.linspace(1,6,6))

"""# Checking distribution of host id and id on basis of neighbourhoods"""

dadaw=dg.groupby(['neighbourhood group'])[[('host id'),('id')]].count()
dadaw.plot(kind='bar')
sns.set(rc={'figure.figsize':(60,30)})
sns.set(font_scale=2)
for i,j in dadaw.items():
  print(i)
  print(j)

"""it is clear that majority of hosts renting out their property on airbnb reside in brooklyn and manhattan"""

dadaw2=dg.groupby(['NAME'])[('host name')].value_counts().sort_values(ascending=True)

for i,j in dadaw2.items():
  print(i)
  print(j)

dg['NAME'] = dg['NAME'].astype(str)
dadaw245 = dg.groupby(['NAME'])['NAME'].value_counts().sort_values(ascending=True)
for i in dadaw245.items():
  print(i)

dg['host name'] = dg['host name'].astype(str)

dadaw132=dg.groupby(['host name'])[('host name')].value_counts().sort_values(ascending=True)

for i in dadaw132.items():
  print(i)

"""***we are now aware of our service's top users and places that are rented out and thus we can take their reviews and their activities on app into consideration and provide recomendations accordingly***"""

